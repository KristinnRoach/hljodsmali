// src/lib/SamplerEngine.ts
import SingleUseVoice from './SingleUseVoice';
import {
  SampleNodes,
  Filter_settings,
  Lock_settings,
  SampleRecord,
  Sample_settings,
  Time_settings,
  Tuning_settings,
  Volume_settings,
  getDefaultSampleSettings,
} from '../../types/samples';

import {
  findZeroCrossings,
  snapToNearestZeroCrossing,
} from '../audio-utils/zeroCrossingUtils';

import {
  FormatKey,
  APP_FORMATS,
  AudioFormat,
} from '../../types/constants/mimeTypes';

/* Singleton class for managing audio playback and recording */

export default class SamplerEngine {
  private static instance: SamplerEngine | null = null;

  private audioCtx: AudioContext;
  private audioFormatKey: FormatKey = 'WEBM';

  /* Currently active voices state */

  private activeVoices: Set<SingleUseVoice> = new Set();

  /* Currently loaded samples state- make sure to clean up resources */
  private sampleIds: Set<string> = new Set();
  private buffers: Map<string, AudioBuffer> = new Map();
  private sampleNodes: Map<string, SampleNodes> = new Map();
  private zeroCrossings: Map<string, number[]> = new Map();
  private sampleSettings: Map<string, Sample_settings> = new Map();

  /* Current selection state */
  private selectedForPlayback: Set<string> = new Set();
  private selectedForSettings: Set<string> = new Set(); // JUST IMPLEMENTED FOR SINGLE SAMPLE

  /* Master Audio Nodes */
  private masterGain: GainNode;
  private output: AudioNode;
  private externalOutputs: Set<AudioNode> = new Set();
  private compressorLimiter: DynamicsCompressorNode;

  /* Constructor */
  private constructor(audioCtx: AudioContext | null) {
    // Todo: Remove audioctx parameter? (use audioctcx utils?)
    if (!audioCtx) {
      throw new Error('Audio context not set up');
    }
    this.audioCtx = audioCtx;
    this.output = this.audioCtx.destination;

    this.masterGain = this.audioCtx?.createGain();
    this.masterGain.gain.value = 0.8;
    this.compressorLimiter = this.initializeCompressorLimiter(
      -3,
      6,
      3,
      0.01,
      0.25
    );
    this.masterGain.connect(this.compressorLimiter);
    this.compressorLimiter.connect(this.output);
    // this.masterGain.connect(this.output); // if need to remove compressorLimiter for debugging

    console.log('Sampler Engine context: ', this.audioCtx);

    console.log('MIME type: ', APP_FORMATS[this.audioFormatKey]);

    SingleUseVoice.initialize(this.audioCtx.sampleRate);
  }

  /* Sample Engine instance */

  public static getInstance(audioCtx?: AudioContext): SamplerEngine {
    if (audioCtx && !SamplerEngine.instance) {
      SamplerEngine.instance = new SamplerEngine(audioCtx);
      console.log('new SamplerEngine instance created');
    } else if (!SamplerEngine.instance) {
      throw new Error('SamplerEngine not initialized');
    }
    return SamplerEngine.instance;
  }

  //__________________________ MASTER BUS __________________________ //

  // TODO: move to audioctx utils - along with all other audioctx related functions
  private initializeCompressorLimiter(
    threshold: number = -10,
    knee: number = 0,
    ratio: number = 20,
    attack: number = 0.003,
    release: number = 0.25
  ): DynamicsCompressorNode {
    const compressor = this.audioCtx.createDynamicsCompressor();
    compressor.threshold.setValueAtTime(threshold, this.audioCtx.currentTime);
    compressor.knee.setValueAtTime(knee, this.audioCtx.currentTime);
    compressor.ratio.setValueAtTime(ratio, this.audioCtx.currentTime);
    compressor.attack.setValueAtTime(attack, this.audioCtx.currentTime);
    compressor.release.setValueAtTime(release, this.audioCtx.currentTime);
    return compressor;
  }

  connectToExternalOutput(destination: AudioNode): void {
    if (!this.externalOutputs.has(destination)) {
      this.masterGain.connect(destination);
      this.externalOutputs.add(destination);
    }
  }

  disconnectExternalOutput(destination: AudioNode): void {
    this.masterGain.disconnect(destination);
    this.externalOutputs.delete(destination);
  }

  /* Loop and Hold state manager */

  toggleLoop(): void {
    SingleUseVoice.toggleLoop();
  }

  public isLooping(): boolean {
    return SingleUseVoice.isLooping();
  }

  toggleHold(): void {
    SingleUseVoice.toggleHold();
  }

  isHolding(): boolean {
    return SingleUseVoice.isHolding();
  }

  setSampleLoopLock(sampleId: string, lock: boolean): void {
    const settings = this.sampleSettings.get(sampleId);
    if (!settings || settings.locks.loop === lock) {
      return;
    } else {
      settings.locks.loop = lock;
      // this.updateActiveLoopLocks(sampleId);
    }
  }

  /* Engine Settings */

  getAudioFormat(): FormatKey {
    return this.audioFormatKey;
  }

  /* Sample Manager */

  createGain(volume: number): GainNode {
    const newGain = this.audioCtx.createGain();
    newGain.gain.value = volume;
    return newGain;
  }

  createFilters(
    lowCutoff?: number,
    highCutoff?: number
  ): { lowCut: BiquadFilterNode; highCut: BiquadFilterNode } {
    const lowCut = this.audioCtx.createBiquadFilter();
    lowCut.type = 'highpass';
    lowCut.frequency.value = lowCutoff || 40;

    const highCut = this.audioCtx.createBiquadFilter();
    highCut.type = 'lowpass';
    highCut.frequency.value = highCutoff || 20000;

    return { lowCut: lowCut, highCut: highCut };
  }

  connectSampleAudioNodes(
    sampleGain: GainNode,
    lowCut: BiquadFilterNode,
    highCut: BiquadFilterNode,
    masterOut: GainNode = this.masterGain
  ): void {
    if (!masterOut) throw new Error('Master output not set up');

    sampleGain.connect(lowCut);
    lowCut.connect(highCut);
    highCut.connect(masterOut);
    masterOut.connect(this.audioCtx.destination);
  }

  loadSample(record: SampleRecord, buffer: AudioBuffer): void {
    const settings = record.sample_settings;
    const zeroCrossings = findZeroCrossings(buffer); // Todo: VALIDATE zeroCrossings is valid

    const sampleGain = this.createGain(settings.volume.sampleVolume);

    const { lowCut, highCut } = this.createFilters(
      settings.filters.lowCutoff,
      settings.filters.highCutoff
    );

    this.connectSampleAudioNodes(sampleGain, lowCut, highCut);
    const sampleNodes = { sampleGain, lowCut, highCut };

    this.sampleIds.add(record.id);
    this.buffers.set(record.id, buffer);
    this.sampleNodes.set(record.id, sampleNodes);
    this.zeroCrossings.set(record.id, zeroCrossings);
    this.sampleSettings.set(record.id, settings);

    // TODO: remove duplication of data !
    SingleUseVoice.zeroCrossings.set(record.id, zeroCrossings);
    SingleUseVoice.sampleSettings.set(record.id, record.sample_settings);

    console.log(
      'Loaded record:',
      record,
      'ids:',
      this.sampleIds,
      'buffers:',
      this.buffers,
      'sampleNodes:',
      this.sampleNodes,
      'zeroCrossings:',
      this.zeroCrossings,
      'sampleSettings:',
      this.sampleSettings
    );
  }

  unloadSample(id: string): void {
    this.zeroCrossings.delete(id);
    this.sampleSettings.delete(id);
    this.buffers.delete(id);
    this.sampleNodes.delete(id);
    this.sampleIds.delete(id);

    // TODO: remove duplication of data !
    SingleUseVoice.sampleSettings.delete(id);
    SingleUseVoice.zeroCrossings.delete(id);
  }

  setSampleVolume(sampleId: string, volume: number) {
    if (!volume || volume < 0 || volume > 1) {
      throw new Error('Invalid volume value');
    }

    if (
      !(this.sampleNodes.has(sampleId) && this.sampleSettings.has(sampleId))
    ) {
      throw new Error('Sample not loaded');
    }
    const gainNode = this.sampleNodes.get(sampleId)?.sampleGain;
    const settings = this.sampleSettings.get(sampleId);

    if (!(gainNode && settings)) return;

    gainNode.gain.setTargetAtTime(volume, this.audioCtx.currentTime, 0.1); // not tested time constant value 0.1 ! Create global variable
    settings.volume.sampleVolume = volume;
  }

  isLoaded(id: string): boolean {
    return (
      this.sampleIds.has(id) &&
      this.buffers.has(id) &&
      this.sampleNodes.has(id) &&
      this.sampleSettings.has(id) &&
      this.zeroCrossings.has(id)
    );
  }

  isSelectedForPlayback(id: string): boolean {
    return this.selectedForPlayback.has(id);
  }

  isSelectedForSettings(id: string): boolean {
    return this.selectedForSettings.has(id);
  }

  selectForPlayback(id: string): void {
    this.selectedForPlayback.add(id);
  }

  deselectForPlayback(id: string): void {
    this.selectedForPlayback.delete(id);
  }

  selectForSettings(id: string): void {
    this.selectedForSettings.add(id);
  }

  deselectForSettings(id: string): void {
    this.selectedForSettings.delete(id);
  }

  getSelectedForPlayback(): string[] {
    return Array.from(this.selectedForPlayback);
  }

  getBufferForPlayback(id: string): AudioBuffer | null {
    return this.buffers.get(id) || null;
  }

  getSelectedForSettings(): string[] {
    if (this.selectedForSettings.size === 1) {
      return [this.selectedForSettings.values().next().value];
    } else {
      return Array.from(this.selectedForSettings);
    }
  }

  getBufferForSettings(id: string): AudioBuffer | null {
    return this.buffers.get(id) || null;
  }

  getSampleSettings(
    id: string,
    type: 'Time' | 'Volume' | 'Tuning' | 'Filter' | 'Lock' | 'All' = 'All'
  ):
    | Time_settings
    | Volume_settings
    | Tuning_settings
    | Filter_settings
    | Lock_settings
    | Sample_settings
    | null {
    const settings = this.sampleSettings.get(id);

    if (!settings) {
      console.error(`Sample ${id} not loaded`);
      return null;
    }

    switch (type) {
      case 'Time':
        return (settings.time as Time_settings) ?? null;

      case 'Volume':
        return (settings.volume as Volume_settings) ?? null;

      case 'Tuning':
        return (settings.tune as Tuning_settings) ?? null;

      case 'Filter':
        return (settings.filters as Filter_settings) ?? null;

      case 'Lock':
        return (settings.locks as Lock_settings) ?? null;

      case 'All':
        return settings as Sample_settings;

      default:
        return settings as Sample_settings;
    }
  }

  isPlaying(): boolean {
    return SingleUseVoice.isPlaying();
  }

  getCurrentPlayheadPosition(): number {
    return SingleUseVoice.getCurrentPlayheadPosition();
  }

  /* Individual Sample Settings */

  updateTimeSettings(id: string, settings: Partial<Time_settings>) {
    // change to key and value
    if (
      !(
        this.selectedForSettings.size === 1 &&
        this.selectedForSettings.has(id) &&
        this.sampleSettings.has(id) &&
        this.zeroCrossings.has(id)
      )
    ) {
      throw new Error('Invalid sample selection for updating time settings');
    }

    const prevSettings = this.sampleSettings.get(id);
    const zeroCrossings = this.zeroCrossings.get(id);

    if (!prevSettings || !zeroCrossings) throw new Error('Sample not loaded');

    let updatedSettings: Time_settings = { ...prevSettings.time };

    for (const [key, value] of Object.entries(settings)) {
      const snapped = snapToNearestZeroCrossing(value, zeroCrossings);
      updatedSettings = { ...updatedSettings, [key]: snapped };
    }

    let newSettings = { ...prevSettings, ...updatedSettings };
    this.sampleSettings.set(id, newSettings);

    SingleUseVoice.sampleSettings.set(id, newSettings);

    SingleUseVoice.updateActiveVoices(id, newSettings);

    console.log('Updated time settings:', newSettings);
  }

  updateEnvelopeSettings(id: string, settings: Partial<Volume_settings>) {
    // change to key and value
    if (
      !(
        this.selectedForSettings.size === 1 &&
        this.selectedForSettings.has(id) &&
        this.sampleSettings.has(id)
      )
    ) {
      throw new Error(
        'Invalid sample selection for updating envelope settings'
      );
    }

    const prevSettings = this.sampleSettings.get(id);

    if (!prevSettings) throw new Error('Sample not loaded');

    let updatedSettings: Volume_settings = { ...prevSettings.volume };

    for (const [key, value] of Object.entries(settings)) {
      updatedSettings = { ...updatedSettings, [key]: value };
    }

    let newSettings = { ...prevSettings, volume: updatedSettings };
    this.sampleSettings.set(id, newSettings);

    SingleUseVoice.sampleSettings.set(id, newSettings);

    SingleUseVoice.updateActiveVoices(id, newSettings);

    console.log('Updated envelope settings:', newSettings);
  }

  /* Playback */

  playNote(midiNote: number): void {
    for (const id of this.selectedForPlayback) {
      const buffer = this.buffers.get(id);
      const sampleNodes = this.sampleNodes.get(id);

      console.log('Playing note:', midiNote, 'for sample:', id);

      if (buffer && sampleNodes) {
        const voice = new SingleUseVoice(this.audioCtx, buffer, id);
        voice.getVoiceGain().connect(sampleNodes.sampleGain);

        voice.start(midiNote);
      }
    }
  }

  releaseNote(midiNote: number): void {
    SingleUseVoice.releaseNote(midiNote);
  }

  stopAllVoices(): void {
    SingleUseVoice.panic();
  }

  /* Master Volume */

  setMasterVolume(volume: number) {
    this.masterGain.gain.setTargetAtTime(
      volume,
      this.audioCtx.currentTime + 0.01,
      0.5
    ); //not tested time constant value
  }

  getMasterVolume(): number {
    if (!this.masterGain.gain.value) {
      throw new Error('Master gain node not set up');
    }
    return this.masterGain.gain.value;
  }
}

// updateSampleSettings(id: string, settings: Partial<Sample_settings>) {
//   // Replace Sample_settings with Time_settings etc..
//   try {
//     if (loadedSample) {
//       if (settings.lowCutoff !== undefined) {
//         loadedSample.sampleNodes.lowCut.frequency.setValueAtTime(
//           settings.lowCutoff,
//           this.audioCtx.currentTime
//         );
//         loadedSample.sample_settings.lowCutoff = settings.lowCutoff;
//       }
//       if (settings.highCutoff !== undefined) {
//         loadedSample.sampleNodes.highCut.frequency.setValueAtTime(
//           settings.highCutoff,
//           this.audioCtx.currentTime
//         );
//         loadedSample.sample_settings.highCutoff = settings.highCutoff;
//       }
//       if (settings.startPoint !== undefined && loadedSample.zeroCrossings) {
//         const snapped = snapToNearestZeroCrossing(
//           settings.startPoint,
//           loadedSample.zeroCrossings
//         );
//         loadedSample.sample_settings.startPoint = snapped;
//       }
//       if (settings.endPoint !== undefined && loadedSample.zeroCrossings) {
//         const snapped = snapToNearestZeroCrossing(
//           settings.endPoint,
//           loadedSample.zeroCrossings
//         );
//         loadedSample.sample_settings.endPoint = snapped;
//       }

//       if (settings.transposition !== undefined) {
//         loadedSample.sample_settings.transposition = settings.transposition;
//       }

//       if (settings.tuneOffset !== undefined) {
//         loadedSample.sample_settings.tuneOffset = settings.tuneOffset;
//       }

//       // const newLoadedSample = {
//       //   // WHY???
//       //   ...loadedSample,
//       //   sample_settings: {
//       //     ...loadedSample.sample_settings,
//       //     ...settings,
//       //   },
//       // };

//       SingleUseVoice.sampleSettings.set(
//         loadedSample.id,
//         loadedSample.sample_settings
//       );

//       console.log(
//         'Updated sample settings:',
//         loadedSample.sample_settings.transposition,
//         loadedSample.sample_settings.tuneOffset,
//         loadedSample.sample_settings.lowCutoff
//       );

//       if (
//         'loopStart' in settings ||
//         'loopEnd' in settings ||
//         'tuneOffset' in settings ||
//         'transposition' in settings ||
//         'sampleVolume' in settings ||
//         'loopVolume' in settings
//       ) {
//         SingleUseVoice.updateActiveVoices(id, settings);
//       }
//     }
//   } catch (error) {
//     console.error(`Error updating sample ${id}:`, error);
//   }
// }

// createSampleFile ?
// createSampleRecord ?

// TODO: implement (with AudioRecorder)
// setAudioFormat(formatKey: FormatKey): void {
//   this.audioFormatKey = formatKey;
// }

// REMOVE REST WHEN FIXED
// const loaded = this.loadedSamples.get(sampleId);
// if (loaded) {
//   loaded.sampleNodes.sampleGain.gain.setValueAtTime(
//     volume,
//     this.audioCtx.currentTime
//   );
//   loaded.sample_settings.sampleVolume = volume;
// }

// isSampleLoaded(id: string): boolean {
//   const sample = this.loadedSamples.get(id);

//   return (
//     (sample && sample.buffer && sample.sampleNodes.sampleGain) !== undefined
//   );
// }

// const loadedSample: LoadedSample = {
//   id: record.id,
//   name: record.name,
//   slug: record.slug,
//   buffer: buffer,
//   zeroCrossings: zeroCrossings,
//   sample_settings: settings,
//   sampleNodes: sampleNodes,
// };

// this.loadedSamples.set(loadedSample.id, loadedSample);

// addBufferDurationToLoadedSamples(): void {
//   // remove function if redundant, else add zeroCrossings
//   this.loadedSamples.forEach((loadedSample, key) => {
//     if (loadedSample.buffer && loadedSample.buffer.duration) {
//       loadedSample.sample.bufferDuration = loadedSample.buffer.duration;
//       this.loadedSamples.set(key, loadedSample);
//     }
//   });
// }

/* Recording */

// onSilence() {
//   console.log('silence, ');
// }
// onSpeak() {
//   console.log('speaking');
// }

// navigator.mediaDevices
//   .getUserMedia({
//     audio: true,
//   })
//   .then((stream) => {
//     detectSilence(stream, onSilence, onSpeak);

// stopSilenceDetection: (() => void) | null = null;

// cleanupSilenceDetection(): void {
//   if (this.stopSilenceDetection) {
//     this.stopSilenceDetection();
//     this.stopSilenceDetection = null;
//   }
// }

// async setupRecording(): Promise<void> {
//   try {
//     const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

//     // this.stopSilenceDetection = detectSilence(
//     //   this.audioCtx,
//     //   stream,
//     //   this.onSilence,
//     //   this.onSpeak
//     // );

//     this.mediaRecorder = new MediaRecorder(stream, {
//       mimeType: this.audioFormat.mimeType,
//     });
//     this.mediaRecorder.ondataavailable = (event) => {
//       if (event.data.size > 0) {
//         this.recordedChunks.push(event.data);
//       }
//     };
//   } catch (error) {
//     console.error('Failed to setup recording:', error);
//     throw error;
//   }
// }

// async startRecording(): Promise<void> {
//   if (!this.mediaRecorder) {
//     await this.setupRecording();
//   }
//   this.recordedChunks = [];
//   this.mediaRecorder?.start();
// }

// async stopRecording(): Promise<Blob> {
//   return new Promise((resolve, reject) => {
//     if (!this.mediaRecorder) {
//       reject(new Error('Media Recorder not set up'));
//     }

//     this.mediaRecorder!.onstop = () => {
//       const blob = new Blob(this.recordedChunks, {
//         type: this.audioFormat.mimeType,
//       });
//       if (!blob || blob.size === 0 || blob.type === '') {
//         reject(new Error('No audio data recorded'));
//       }
//       // this.cleanupSilenceDetection();
//       resolve(blob);
//     };

//     this.mediaRecorder!.stop();
//   });
// }

// export function createNewLoadedSample(
//   id: string,
//   name: string,
//   slug: string,
//   buffer: AudioBuffer
// ): LoadedSample {
//   const duration = buffer.duration;
//   const zeroCrossings = findZeroCrossings(buffer);
//   const defaultSettings = getDefaultSampleSettings(duration);

//   return {
//     id: id,
//     name: name,
//     slug: slug,
//     buffer: buffer,
//     zeroCrossings: zeroCrossings,
//     sample_settings: defaultSettings,
//   };
// }

// async stopRecording(): Promise<{
//   sampleRecord: SampleRecord;
//   buffer: AudioBuffer;
// }> {
//   if (!this.mediaRecorder) {
//     throw new Error('Media Recorder not set up');
//   }

//   return new Promise((resolve, reject) => {
//     this.mediaRecorder!.onstop = async () => {
//       try {
//         const blob = new Blob(this.recordedChunks, { type: 'audio/webm' }); // decide blob / string / file
//         const arrayBuffer = await blob.arrayBuffer();
//         const audioBuffer = await this.audioCtx.decodeAudioData(arrayBuffer);

//         const sampleSettings = getDefaultSampleSettings(audioBuffer.duration);

//         // const savedRecord: SampleRecord = await saveNewSampleRecord(
//         //   'new-sample' + new Date().getHours() + new Date().getMinutes(),
//         //   blob,
//         //   sampleSettings
//         // );

//         // const timeOfRecording = new Date().getHours()+'-'+new Date().getMinutes();

//         // const record: SampleRecord = {
//         //   id: 'new-sample' + Date.now().toString(),
//         //   name: 'new-sample' + '-' + timeOfRecording,
//         //   slug: 'new-sample' + '-' + timeOfRecording,
//         //   sample_file: blob,
//         //   created: new Date().toISOString(),
//         //   updated: new Date().toISOString(),
//         //   sample_settings: sampleSettings,
//         // };

//         // try {
//         //   await saveNewSampleRecord(
//         //     record.name,
//         //     record.sample_file,
//         //     record.sample_settings
//         //   );
//         // } catch (error) {
//         //   console.error('Error saving new sample record:', error);
//         //   throw error;
//         // }

//         // this.loadSample(record, audioBuffer);

//         this.recordedChunks = [];
//         resolve({ sampleRecord: record, buffer: audioBuffer }); // could be void?
//         // this.setupRecording(); // Reset media recorder here?
//       } catch (error) {
//         reject(error);
//       }
//     };

//     this.mediaRecorder!.stop();
//   });
// }

// getNewRecordings(): Sample_db[] {
//   const newRecordings = [...this.newRecordedSamples];
//   this.newRecordedSamples = [];
//   return newRecordings;
// }

// hasNewRecordings(): boolean {
//   return this.newRecordedSamples.length > 0;
// }

// // Move to sample type file, localize creation of sample_db objects to one place for consistency

// createNewSampleObject(name: string, blob: Blob, duration: number): Sample_db {
//   const file = new File([blob], name + '.webm', { type: 'audio/webm' }); // check for consistency
//   const slug = name.toLowerCase().replace(/ /g, '-');

//   const defaultSettings = getDefaultSampleSettings(duration);

//   const sample: Sample_db = {
//     id: `new-sample: ${this.newRecordedSamples.length + 1}`,
//     name: name,
//     slug: slug,
//     user: 'user', // Add user ID
//     sample_file: file + '',
//     created: new Date().toISOString(),
//     updated: new Date().toISOString(),
//     bufferDuration: duration,
//     sample_settings: defaultSettings,
//   };
//   return sample;
// }

//   loadSample(sample: Sample, buffer: AudioBuffer): LoadedSample {
//     // should be void?
//     const defaultSettings = this.getDefaultSampleSettings(buffer.duration);

//     // Set default values if not present
//     const updatedSample: Sample = {
//       ...sample,
//       bufferDuration: buffer.duration,
//       sample_settings: sample.sample_settings
//         ? {
//             ...defaultSettings,
//             ...sample.sample_settings,
//           }
//         : defaultSettings,
//     };

//     // prep gain node for individual sample volume control
//     const sampleGain = this.audioContext.createGain();
//     sampleGain.connect(this.masterGain);
//     sampleGain.gain.value =
//       sample.sample_settings?.sampleVolume ?? this.masterGain.gain.value;

//     const loadedSample: LoadedSample = {
//       sample: updatedSample,
//       buffer: buffer,
//       sampleGain: sampleGain,
//     };

//     this.loadedSamples.set(sample.id, loadedSample);
//     // Set the newly loaded sample as the only selected sample
//     this.setSelectedSampleIds([sample.id]);
//     return loadedSample;
//   }

//   releaseAllLoops(): void {
//     const activeVoiceKeys = Array.from(this.activeVoices.keys());

//     activeVoiceKeys.forEach((voiceKey) => {
//       const voice = this.activeVoices.get(voiceKey);

//       if (voice && voice.getLoop()) {
//         voice.setLoop(false);
//         // voice.release();
//         this.activeVoices.delete(voiceKey);
//       }
//     });
//     console.log('releaseAllLoops called. Active voices: ', this.activeVoices);
//   }

//   playNote(midiNote: number, isLoopOn: boolean): void {
//     const selected = this.getSelectedLoadedSamples();
//     selected.forEach((loadedSample) => {
//       if (loadedSample) {
//         // loadedSample.sample.sample_settings && loadedSample.buffer && loadedSample.sampleGain
//         const voice = new SingleUseVoice(
//           this.audioContext,
//           isLoopOn,
//           loadedSample.sample.sample_settings.sampleVolume,
//           loadedSample.buffer,
//           loadedSample.sample.sample_settings
//         );
//         voice.voiceGain.connect(loadedSample.sampleGain);
//         loadedSample.sampleGain.connect(this.masterGain);
//         this.masterGain.connect(this.audioContext.destination);
//         voice.start(
//           midiNote
//           // this.masterGain.gain.value,
//           // loadedSample.sample.sample_settings.attackTime
//         );

//         const voiceKey = `${loadedSample.sample.id}-${midiNote}`;
//         this.activeVoices.set(voiceKey, voice);
//       }
//     });
//   }

//   releaseNote(midiNote: number, isLoopOn: boolean): void {
//     const selectedSamples = this.getSelectedLoadedSamples();
//     selectedSamples.forEach((loadedSample) => {
//       const voiceKey = `${loadedSample.sample.id}-${midiNote}`;
//       const voice = this.activeVoices.get(voiceKey);
//       if (voice) {
//         voice.release(); // loadedSample.sample.sample_settings.releaseTime

//         if (voice.getLoop()) {
//           // ||Â isLoopOn ?
//           // if (loadedSample.sampleGain.gain.value !== voice.loopVolume)
//           //   loadedSample.sampleGain.gain.setValueAtTime(
//           //     loadedSample.sampleGain.gain.value,
//           //     this.audioContext.currentTime // + voice.releaseTime
//           //   );
//           // loadedSample.sampleGain.gain.linearRampToValueAtTime(
//           //   voice.loopVolume,
//           //   this.audioContext.currentTime + voice.releaseTime
//           // );
//         } else {
//           // voice.release(); // loadedSample.sample.sample_settings.releaseTime

//           // Wait for release time before removing voice from activeVoices // is this necessary?
//           setTimeout(() => {
//             this.activeVoices.delete(voiceKey);
//           }, voice.releaseTime * 1000);
//         }
//       }
//     });
//   }
// updateActiveLoopLocks(id?: string): void {
//   this.loadedSamples.forEach((s) => {
//     if (id && s.sample.id === id) {
//       return;
//     }
//     if (s.sample.sample_settings.loopLocked) {
//       this.updateActiveVoiceSettings(s.sample.id, s.sample.sample_settings);
//     }
//   });
// }

// selectForPlayback(ids: string[]): void {
//   this.selectedForPlayback = new Set(ids);

//   ids.forEach((id) => {
//     if (this.loadedSamples.has(id)) {
//       this.selectedForPlayback.add(id);
//     } else {
//       console.error(`Sample ${id} not loaded`);
//     }
//   });
// }

// deselectForPlayback(ids: string[]): void {
//   ids.forEach((id) => {
//     this.selectedForPlayback.delete(id);
//   });
// }

// selectForSettings(ids: string[]): void {
//   this.selectedForSettings = new Set(ids);

//   ids.forEach((id) => {
//     if (this.loadedSamples.has(id)) {
//       this.selectedForSettings.add(id);
//     } else {
//       console.error(`Sample ${id} not loaded`);
//     }
//   });
// }

// type LoadedSample = {
//   // TODO: Remove this. Use buffer[], zeroCrossings[], sample_settings[] etc.. instead
//   id: string;
//   name: string;
//   slug: string;

//   buffer: AudioBuffer;
//   zeroCrossings: number[];
//   sample_settings: Sample_settings;

//   sampleNodes: SampleNodes;
// };
// private loadedSamples: Map<string, LoadedSample> = new Map(); // REMOVE!
