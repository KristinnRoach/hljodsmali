// src/lib/engine/SingleUseVoice.ts
import SampleSettingsManager from './managers/SettingsManager';
import LoopHoldManager from './managers/LoopHoldManager';
import ZeroCrossingManager from './managers/ZeroCrossingManager';

import {
  AmpEnv,
  Pitch_settings,
  Sample_settings,
  Time_settings,
} from '../types/samples';

import { MIDDLE_C_MIDI } from '../types/constants/constants';

import {
  snapDurationToNote,
  interpolateDurationToNote,
  C5_DURATION_SEC,
} from '../types/constants/note-utils';

export default class SingleUseVoice {

  private source: AudioBufferSourceNode;
  private voiceGain: GainNode;

  private startPoint: number = 0;
  private endPoint: number = 0;
  private playDuration: number = 0;

  private isPlaying: boolean = false;
  private midiNote: number = -1;
  private trigger: number = -1;

  constructor(
    private audioCtx: AudioContext,
    readonly buffer: AudioBuffer,
    private sampleId: string
  ) {
    const sampleManager = SampleSettingsManager.getInstance();
    const settings = sampleManager.getSampleSettings(
      sampleId
    ) as Sample_settings;

    if (!settings) {
      throw new Error('Sample not loaded, id: ' + sampleId);
    }

    this.initialize(settings);
  }

  private initialize(settings: Sample_settings): void {
    this.source = this.audioCtx.createBufferSource();
    this.source.buffer = this.buffer;
    this.voiceGain = this.audioCtx.createGain();
    this.voiceGain.gain.value = 0;
    this.source.connect(this.voiceGain);

    this.startPoint = settings.time.startPoint;
    this.endPoint = settings.time.endPoint;
    this.playDuration = this.endPoint - this.startPoint;

    const loopHoldManager = LoopHoldManager.getInstance();
    this.source.loop = loopHoldManager.globalLoop || settings.locks.loop;
    if (this.source.loop) {
      this.source.loopStart = settings.time.loopStart;
      this.source.loopEnd = settings.time.loopEnd;
    }

    this.source.onended = () => {
      this.stop();
    };
  }

  // ____________  GETTERS  ____________

  getSampleId(): string {
    return this.sampleId;
  }

  getVoiceGain(): GainNode {
    return this.voiceGain;
  }

  getTriggerTime(): number {
    return this.trigger;
  }

  getMidiNote(): number {
    return this.midiNote;
  }

  // ____________  SETTERS  ___________________

  setLoopStart(loopStart: number): void {
    this.source.loopStart = loopStart;
  }

  setLoopEnd(loopEnd: number): void {
    this.source.loopEnd = loopEnd;
  }

  private setPlaybackRate(
    // move parts to constructor or voice manager to seperate concerns ?
    midiNote: number,
    pitchSettings: Pitch_settings
  ): void {
    const midiRate = this.semitoneToRate(1, midiNote - MIDDLE_C_MIDI);
    const transposedRate = this.semitoneToRate(
      midiRate,
      pitchSettings.transposition
    );
    const tunedRate = this.semitoneToRate(
      transposedRate,
      pitchSettings.tuneOffset
    );
    this.source.playbackRate.value = tunedRate;

    this.playDuration = this.playDuration / this.source.playbackRate.value; // seperate concerns
  }

  // ____________  METHODS  ___________________

  start(midiNote: number): void {
    const settingsManager = SampleSettingsManager.getInstance(); // simplify - localize
    const pitchSettings = settingsManager.getSampleSettings(
      this.sampleId,
      'pitch'
    ) as Pitch_settings;

    this.setPlaybackRate(midiNote, pitchSettings); // also sets playDuration

    this.startSource();

    this.triggerAttack();
    this.midiNote = midiNote; // using?
  }

  private startSource(): void {
    this.source.start(
      0,
      this.startPoint,
      this.source.loop
        ? undefined // no endPoint is set if is looping
        : this.playDuration // stops the source after the duration of the sample
    );

    this.isPlaying = true;
  }

  stop(): void {
    this.source.stop();
    this.source.disconnect();
    this.voiceGain.disconnect();
    this.isPlaying = false;

    // SingleUseVoice.allVoices.delete(this);
  }

  triggerAttack(): void {
    const sampleManager = SampleSettingsManager.getInstance();
    const settings = sampleManager.getSampleSettings(
      this.sampleId
    ) as Sample_settings;

    this.voiceGain.gain.linearRampToValueAtTime(
      settings.volume.sampleVolume,
      this.audioCtx.currentTime + settings.ampEnv.attackTime
    );
    this.trigger = this.audioCtx.currentTime;
  }

  triggerRelease(): void {
    const globalState = LoopHoldManager.getInstance();
    if (this.trigger <= 0 || globalState.hold) return;

    // just pass in the value from the context? Simplify communication between classes
    const sampleManager = SampleSettingsManager.getInstance();
    const settings = sampleManager.getSampleSettings(
      this.sampleId,
      'ampEnv'
    ) as AmpEnv;

    this.voiceGain.gain.linearRampToValueAtTime(
      0,
      this.audioCtx.currentTime +
        settings.releaseTime / this.source.playbackRate.value
    );

    this.source.stop(this.audioCtx.currentTime + settings.releaseTime + 1);
  }

  private semitoneToRate(baseRate: number, semitones: number): number {
    return baseRate * Math.pow(2, semitones / 12);
  }

  updateLoopPoints(
    newStart: number,
    newEnd: number,
    prevStart: number,
    prevEnd: number
  ): void {
    const newLoopLength = newEnd - newStart;
    if (newLoopLength <= C5_DURATION_SEC) return; // lowest allowed value

    if (newLoopLength > 0.015) {
      this.setLoopStart(newStart);
      this.setLoopEnd(newEnd);
      return;
    }

    // ... refactor this part to a helper function

    const prevLoopLength = prevEnd - prevStart;

    const nearestNote = snapDurationToNote(
      newLoopLength,
      ['C'],
      'C',
      'C',
      0,
      7,
      'sec'
    );

    interpolateDurationToNote(
      prevLoopLength,
      nearestNote,
      ['C'],
      'C',
      'C',
      0,
      7,
      'sec',
      500, // animation duration in ms,
      (interpolatedLength: number) => {
        const adjustedNewEnd = newStart + interpolatedLength;
        this.setLoopStart(newStart);
        this.setLoopEnd(adjustedNewEnd);
      }
    );
  }
}

// static initialize(sampleRate: number = 48000): void {
//   SingleUseVoice.sampleRate = sampleRate;
// }

// static isPlaying(): boolean {
//   return SingleUseVoice.allVoices.size > 0;
// }

// static getCurrentPlayheadPosition(): number {
//   if (SingleUseVoice.allVoices.size === 0) return 0;
//   const voice = Array.from(SingleUseVoice.allVoices)[0];
//   return voice.trigger > 0 ? voice.audioCtx.currentTime - voice.trigger : 0;
// }

// static panic(): void {
//   SingleUseVoice.allVoices.forEach((voice) => voice.stop());
//   LoopHoldManager.getInstance().globalLoop = false;
//   SingleUseVoice.allVoices.clear();
// }

// static releaseNote(midiNote: number): void {
//   const globalState = LoopHoldManager.getInstance();
//   SingleUseVoice.allVoices.forEach((voice) => {
//     if (voice.midiNote === midiNote && !globalState.hold) {
//       voice.triggerRelease();
//     }
//   });
// }

// Let's refactor our SingleUseVoice so that a new VoiceManager will handle all the static stuff, keeping SingleUseVoice clean and simple.

// export default class SingleUseVoice {
//   private static sampleRate: number = 48000;
//   private static allVoices: Set<SingleUseVoice> = new Set();
//   private static zeroCrossings: Map<string, number[]> = new Map();
//   private static sampleSettings: Map<string, Sample_settings> = new Map();

//   private static globalLoop: boolean = false;
//   private static hold: boolean = false;

//   private source: AudioBufferSourceNode;
//   private sampleId: string;
//   private midiNote: number = -1;
//   private voiceGain: GainNode;
//   private settings: Sample_settings;
//   private trigger: number = -1;

//   constructor(
//     private audioCtx: AudioContext,
//     readonly buffer: AudioBuffer,
//     sampleId: string
//   ) {
//     if (!SingleUseVoice.sampleSettings.has(sampleId)) {
//       throw new Error('Sample not loaded, id: ' + sampleId);
//     }

//     this.settings = SingleUseVoice.sampleSettings.get(sampleId)!;
//     SingleUseVoice.allVoices.add(this);

//     this.source = this.audioCtx.createBufferSource();
//     this.source.buffer = buffer;
//     this.voiceGain = this.audioCtx.createGain();
//     this.voiceGain.gain.value = 0;
//     this.source.connect(this.voiceGain);

//     this.source.loop = SingleUseVoice.globalLoop || this.settings.locks.loop;

//     this.sampleId = sampleId;
//     this.setLoop();

//     this.source.onended = () => {
//       this.stop();
//     };
//   }

//   static initialize(sampleRate: number = 48000) {
//     SingleUseVoice.sampleRate = sampleRate;
//   }

//   static isPlaying(): boolean {
//     return SingleUseVoice.allVoices.size > 0;
//   }

//   static getCurrentPlayheadPosition() {
//     if (SingleUseVoice.allVoices.size === 0) return 0;
//     const voice = Array.from(SingleUseVoice.allVoices)[0];
//     return voice.trigger > 0 ? voice.audioCtx.currentTime - voice.trigger : 0;
//   }

//   static panic() {
//     SingleUseVoice.allVoices.forEach((voice) => voice.stop());
//     SingleUseVoice.globalLoop = false;
//     SingleUseVoice.allVoices.clear();
//   }

//   static toggleLoop() {
//     SingleUseVoice.globalLoop = !SingleUseVoice.globalLoop;
//     SingleUseVoice.allVoices.forEach((voice) => voice.setLoop());
//   }

//   static isLooping() {
//     return SingleUseVoice.globalLoop;
//   }

//   static toggleHold() {
//     SingleUseVoice.hold = !SingleUseVoice.hold;
//     if (!SingleUseVoice.hold) {
//       SingleUseVoice.allVoices.forEach((voice) => voice.triggerRelease());
//     }
//   }

//   static isHolding() {
//     return SingleUseVoice.hold;
//   }

//   getVoiceGain() {
//     return this.voiceGain;
//   }

//   static releaseNote(midiNote: number) {
//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.midiNote === midiNote && !SingleUseVoice.hold) {
//         voice.triggerRelease();
//       }
//     });
//   }

//   start(midiNote: number) {
//     const midiRate = this.semitoneToRate(1, midiNote - MIDDLE_C_MIDI);
//     const transposedRate = this.semitoneToRate(
//       midiRate,
//       this.settings.tune.transposition
//     );
//     const tunedRate = this.semitoneToRate(
//       transposedRate,
//       this.settings.tune.tuneOffset
//     );
//     this.source.playbackRate.value = tunedRate;

//     this.source.start(
//       0,
//       this.settings.time.startPoint,
//       this.source.loop
//         ? undefined
//         : this.settings.time.endPoint - this.settings.time.startPoint
//     );

//     this.triggerAttack();
//     this.midiNote = midiNote;
//   }

//   stop() {
//     this.source.stop();
//     SingleUseVoice.allVoices.delete(this);
//     this.source.disconnect();
//     this.voiceGain.disconnect();
//   }

//   triggerAttack() {
//     this.voiceGain.gain.linearRampToValueAtTime(
//       this.settings.volume.sampleVolume,
//       this.audioCtx.currentTime + this.settings.volume.attackTime
//     );
//     this.trigger = this.audioCtx.currentTime;
//   }

//   triggerRelease() {
//     if (this.trigger <= 0 || SingleUseVoice.hold) return;

//     this.voiceGain.gain.linearRampToValueAtTime(
//       0,
//       this.audioCtx.currentTime +
//         this.settings.volume.releaseTime / this.source.playbackRate.value
//     );

//     this.source.stop(
//       this.audioCtx.currentTime + this.settings.volume.releaseTime + 1
//     );
//   }

//   setLoop(isLoopOn: boolean = SingleUseVoice.globalLoop) {
//     if (this.settings.locks.loop) return;

//     if (!isLoopOn) {
//       this.triggerRelease();
//     }

//     this.source.loop = isLoopOn;
//     this.calculateLoopPoints();
//   }

//   static updateActiveVoices(
//     sampleId: string,
//     settings: Partial<Sample_settings>
//   ) {
//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.sampleId === sampleId) {
//         voice.settings = { ...voice.settings, ...settings };
//         SingleUseVoice.sampleSettings.set(sampleId, voice.settings);
//         voice.calculateLoopPoints();
//       }
//     });
//   }

//   static tuneActiveVoices(sampleId: string, tuneOffset: number) {
//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.sampleId === sampleId) {
//         voice.settings.tune.tuneOffset = tuneOffset;
//         voice.source.playbackRate.value = voice.semitoneToRate(
//           voice.source.playbackRate.value,
//           tuneOffset
//         );
//       }
//     });
//   }

//   static transposeActiveVoices(sampleId: string, transposition: number) {
//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.sampleId === sampleId) {
//         voice.settings.tune.transposition = transposition;
//         voice.source.playbackRate.value = voice.semitoneToRate(
//           voice.source.playbackRate.value,
//           transposition
//         );
//       }
//     });
//   }

//   private calculateLoopPoints() {
//     const { loopStart, loopEnd } = this.settings.time;
//     if (!loopStart || !loopEnd) return;

//     const initLoopLength = loopEnd - loopStart;
//     if (initLoopLength <= C5_DURATION_SEC) return;

//     const zeroCrossings = SingleUseVoice.zeroCrossings.get(this.sampleId) ?? [];
//     let start = snapToNearestZeroCrossing(loopStart, zeroCrossings);
//     let end = snapToNearestZeroCrossing(loopEnd, zeroCrossings);

//     const zeroSnapLength = end - start;
//     if (zeroSnapLength > 0.015) {
//       this.updateLoopPoints(start, end);
//       return;
//     }

//     const snappedLength = snapDurationToNote(
//       zeroSnapLength,
//       ['C', 'G'],
//       'C',
//       'C',
//       0,
//       7,
//       'sec'
//     );
//     end = start + snappedLength;

//     this.updateLoopPoints(start, end);
//   }

//   private updateLoopPoints(loopStart: number, loopEnd: number) {
//     this.source.loopStart = loopStart;
//     this.source.loopEnd = loopEnd;
//   }

//   private semitoneToRate(baseRate: number, semitones: number): number {
//     return baseRate * Math.pow(2, semitones / 12);
//   }
// }

/* BEFORE REFACTORING */

// // src/lib/engine/SingleUseVoice.ts

// import { Sample_settings, Time_settings } from '../../types/samples';
// import { snapToNearestZeroCrossing } from '../audio-utils/zeroCrossingUtils';
// import {
//   snapDurationToNote,
//   C5_DURATION_SEC,
// } from '../../types/constants/note-utils';
// import { MIDDLE_C_MIDI } from '../../types/constants/constants';

// export default class SingleUseVoice {
//   private static sampleRate: number = 48000;
//   private static allVoices: Set<SingleUseVoice> = new Set();
//   static zeroCrossings: Map<string, number[]> = new Map(); // gera private og setter ef Ã¾etta virkar
//   static sampleSettings: Map<string, Sample_settings> = new Map();

//   // Loop & Hoild: should be able to save for each sample or global? // global makes sense with CapsLock
//   private static globalLoop: boolean = false;
//   private static hold: boolean = false;

//   // private static sampleGainNodesMap: Map<string, GainNode> = new Map();

//   private source: AudioBufferSourceNode;
//   private sampleId: string;
//   private midiNote: number = -1;
//   private voiceGain: GainNode;
//   private settings: Sample_settings;
//   private trigger: number = -1;
//   private held: number = -1;

//   // Static initializer block ?
//   static initialize(sampleRate: number = 48000) {
//     // ensure this always represents the actual sample rate !!!
//     SingleUseVoice.sampleRate = sampleRate;
//   }

//   constructor(
//     private audioCtx: AudioContext,
//     readonly buffer: AudioBuffer,
//     sampleId: string
//   ) {
//     if (!SingleUseVoice.sampleSettings.has(sampleId)) {
//       throw new Error('Sample not loaded, id: ' + sampleId);
//     }

//     this.settings = SingleUseVoice.sampleSettings.get(sampleId)!;
//     SingleUseVoice.allVoices.add(this);

//     this.source = this.audioCtx.createBufferSource();
//     this.source.buffer = buffer;
//     this.voiceGain = this.audioCtx.createGain();
//     this.voiceGain.gain.value = 0;
//     this.source.connect(this.voiceGain);

//     this.source.loop = SingleUseVoice.globalLoop || this.settings.locks.loop;

//     this.sampleId = sampleId;
//     this.setLoop();
//     // this.calculateLoopPoints(); // should be set in setLoop and updateActiveVoices // remove when tested

//     this.source.onended = () => {
//       this.stop();
//     };
//   }

//   /*  SHOULD USE 'SingleUseVoice.sampleGainNodesMap' or 'this.sampleGainNodesMap' for static stuff ?? */

//   static isPlaying(): boolean {
//     return SingleUseVoice.allVoices.size > 0;
//   }

//   static getCurrentPlayheadPosition() {
//     if (!(SingleUseVoice.allVoices.size > 0)) return 0;
//     const voices = Array.from(SingleUseVoice.allVoices);
//     if (voices[0].trigger <= 0) return 0;

//     return voices[0].now() - voices[0].trigger;
//   }

//   static panic() {
//     console.log('Panic! allVoices:', SingleUseVoice.allVoices);
//     SingleUseVoice.allVoices.forEach((voice) => voice.stop());
//     SingleUseVoice.globalLoop = false;
//     SingleUseVoice.allVoices.clear();
//   }

//   static toggleLoop() {
//     // if (SingleUseVoice.globalLoop === isLoopOn) return;

//     SingleUseVoice.globalLoop = !SingleUseVoice.globalLoop;
//     // better to not set for active voices ?
//     SingleUseVoice.allVoices.forEach((voice) => voice.setLoop());
//     // setTimeout(() => {
//     //   console.log('all voices: ', SingleUseVoice.allVoices);
//     //   // SingleUseVoice.allVoices.clear();
//     // }, 500);
//   }

//   static isLooping() {
//     return SingleUseVoice.globalLoop;
//   }

//   static toggleHold() {
//     SingleUseVoice.hold = !SingleUseVoice.hold;
//     if (!SingleUseVoice.hold) {
//       SingleUseVoice.allVoices.forEach((voice) => {
//         voice.triggerRelease();
//       });
//     }
//   }

//   static isHolding() {
//     return SingleUseVoice.hold;
//   }

//   static releaseNote(midiNote: number) {
//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.midiNote === midiNote) {
//         if (!SingleUseVoice.hold) {
//           // (!voice.source.loop) {
//           voice.triggerRelease();
//         }
//       }
//     });
//   }

//   now() {
//     return this.audioCtx.currentTime;
//   }

//   getVoiceGain() {
//     return this.voiceGain;
//   }

//   semitoneToRate(baseRate: number, semitones: number): number {
//     return baseRate * 2 ** (semitones / 12);
//   }

//   start(midiNote: number) {
//     const midiRate = this.semitoneToRate(1, midiNote - MIDDLE_C_MIDI);
//     const transposedRate = this.semitoneToRate(
//       midiRate,
//       this.settings.tune.transposition
//     );
//     const tunedRate = this.semitoneToRate(
//       transposedRate,
//       this.settings.tune.tuneOffset
//     );
//     this.source.playbackRate.value = tunedRate;

//     this.source.start(
//       0,
//       this.settings.time.startPoint,
//       this.source.loop // sets end point only if not looping
//         ? undefined
//         : this.settings.time.endPoint - this.settings.time.startPoint
//     );

//     // if (this.source.loop) {
//     //   this.loopEnvelope();
//     // } else {
//     this.triggerAttack();
//     // }
//     this.midiNote = midiNote;
//   }

//   stop() {
//     this.source.stop();
//     SingleUseVoice.allVoices.delete(this);
//     this.source.disconnect();
//     this.voiceGain.disconnect();
//   }

//   loopEnvelope() {
//     const { attackTime, releaseTime } = this.settings.volume;
//     const { loopStart, loopEnd } = this.settings.time;
//     const loopDuration = loopEnd - loopStart;
//     const now = this.now();
//     this.voiceGain.gain.setValueAtTime(0, now);
//     this.voiceGain.gain.linearRampToValueAtTime(1, now + attackTime);
//     this.voiceGain.gain.setValueAtTime(1, now + loopDuration - releaseTime);
//     this.voiceGain.gain.linearRampToValueAtTime(0, now + loopDuration);
//   }

//   triggerAttack() {
//     this.voiceGain.gain.linearRampToValueAtTime(
//       this.settings.volume.sampleVolume,
//       this.now() + this.settings.volume.attackTime
//     );
//     this.trigger = this.now();
//     // if (this.source.loop) {
//     //   this.triggerRelease();
//     // }
//   }

//   triggerRelease() {
//     if (this.trigger <= 0) return;
//     if (SingleUseVoice.hold) return;

//     this.voiceGain.gain.linearRampToValueAtTime(
//       0,
//       this.now() +
//         this.settings.volume.releaseTime / this.source.playbackRate.value
//     );
//     this.held = this.now() - this.trigger; // not using !!

//     this.source.stop(this.now() + this.settings.volume.releaseTime + 1);
//   }

//   setLoop(isLoopOn: boolean = SingleUseVoice.globalLoop) {
//     if (this.settings.locks.loop) return;

//     if (isLoopOn) {
//       this.calculateLoopPoints();
//     } else {
//       this.triggerRelease(); // get remaining play time ??
//     }

//     this.source.loop = isLoopOn;
//   }

//   setLoopLock(isLocked: boolean) {
//     this.settings.locks.loop = isLocked;
//   }

//   static updateActiveVoices(
//     sampleId: string,
//     settings: Partial<Sample_settings>
//   ) {
//     // FOR EACH SAMPLE Static: update static sample_settings and recalculate loop points. Update Sample_db object in engine and context. Call the function below for each active voice
//     // FOR EACH Active VOICE non-static: use the recalculated loop points to update source.loopStart, source.loopEnd (settings.loopStart, settings.loopEnd needed or no?)

//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.sampleId === sampleId) {
//         voice.calculateLoopPoints();

//         voice.settings = { ...voice.settings, ...settings };
//         SingleUseVoice.sampleSettings.set(sampleId, voice.settings); // make clearer what is going on. No need for both voice and sample settings!
//       }
//     });
//   }

//   static tuneActiveVoices(sampleId: string, tuneOffset: number) {
//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.sampleId === sampleId) {
//         voice.settings.tune.tuneOffset = tuneOffset;
//         voice.source.playbackRate.value = voice.semitoneToRate(
//           voice.source.playbackRate.value,
//           tuneOffset
//         );
//       }
//     });
//   }

//   static transposeActiveVoices(sampleId: string, transposition: number) {
//     SingleUseVoice.allVoices.forEach((voice) => {
//       if (voice.sampleId === sampleId) {
//         voice.settings.tune.transposition = transposition;
//         voice.source.playbackRate.value = voice.semitoneToRate(
//           voice.source.playbackRate.value,
//           transposition
//         );
//       }
//     });
//   }

//   updateLoopPoints(
//     newLoopStart: number = this.source.loopStart,
//     newLoopEnd: number = this.source.loopEnd
//   ) {
//     this.source.loopStart = newLoopStart;
//     this.source.loopEnd = newLoopEnd;
//   }

//   calculateLoopPoints(updated: Partial<Time_settings> = this.settings.time) {
//     // SHOULD BE STATIC?!
//     let loopStart = updated.loopStart;
//     let loopEnd = updated.loopEnd;

//     if (!loopStart || !loopEnd) return; // Temp fix

//     const initLoopLength = loopEnd - loopStart;
//     // console.log('init looplength:', initLoopLength);

//     if (initLoopLength <= C5_DURATION_SEC) return; // how does this affect rendering? Should be in render function, or both?

//     loopStart = snapToNearestZeroCrossing(
//       loopStart,
//       SingleUseVoice.zeroCrossings.get(this.sampleId) ?? []
//     );
//     loopEnd = snapToNearestZeroCrossing(
//       loopEnd,
//       SingleUseVoice.zeroCrossings.get(this.sampleId) ?? []
//     );

//     const zeroSnapLength = loopEnd - loopStart;
//     if (initLoopLength !== zeroSnapLength) {
//       // console.log('length SNAPPED TO ZERO: ', end - start);
//     }

//     this.updateLoopPoints(loopStart, loopEnd);

//     if (zeroSnapLength > 0.015) return;

//     // Snap to notes when in audiorange
//     const snappedLength = snapDurationToNote(
//       zeroSnapLength,
//       ['C', 'G'], // interpolate rather! // , 'D', 'E', 'F', 'G', 'A', 'B'
//       'C',
//       'C',
//       0,
//       7,
//       'sec' // try ms or samples if needed
//     );
//     const newEnd = loopStart + snappedLength;

//     this.source.loopEnd = newEnd; // could call updateLoopPoints again, but only need to update loopEnd to adjust the length

//     console.log('length SNAPPED TO C: ', newEnd - loopStart);
//   }

//   setLoopVolume(volume: number) {
//     this.settings.volume.loopVolume = volume;
//   }
// }

// static updateSampleSettings( // if set does not work in samplerengine
//   sampleId: string,
//   settings: Partial<Sample_settings>
// ) {
//   SingleUseVoice.sampleSettings.set(sampleId, {
//     ...SingleUseVoice.sampleSettings.get(sampleId)!,
//     ...settings,
//   });
//   SingleUseVoice.updateActiveVoices(sampleId, settings);
// }

// C0 (16.35 Hz):
// (1 / 16.35) * 1000 = 61.1620795107034 ms
// C1 (32.70 Hz):
// (1 / 32.70) * 1000 = 30.5810397553517 ms
// C2 (65.41 Hz):
// (1 / 65.41) * 1000 = 15.2881816237577 ms
// C3 (130.81 Hz):
// (1 / 130.81) * 1000 = 7.64468311290726 ms
// C4 (261.63 Hz):
// (1 / 261.63) * 1000 = 3.82234155638498 ms
// C5 (523.25 Hz):
// (1 / 523.25) * 1000 = 1.91117077819399 ms
// C6 (1046.50 Hz):
// (1 / 1046.50) * 1000 = 0.955585389097005 ms
// C7 (2093.00 Hz):
// (1 / 2093.00) * 1000 = 0.477792694455503 ms

// // private minLoopLength: number | null = null;
// private static readonly C1_FREQUENCY = 32.7; // C1 frequency in Hz
// private static minLoopLengthInSamples: number;

// private static calculateMinLoopLength(sampleRate: number): number {
//   const periodInSeconds = 1 / SingleUseVoice.C1_FREQUENCY;
//   return Math.ceil(periodInSeconds * sampleRate);
// }

// Static initializer block (supported in modern JavaScript/TypeScript)
// static initialize(sampleRate: number = 48000) {
//   SingleUseVoice.sampleRate = sampleRate;
// SingleUseVoice.minLoopLengthInSamples =
//   SingleUseVoice.calculateMinLoopLength(sampleRate);
// }

// // Method to update minLoopLengthInSamples if a different sample rate is encountered
// static updateMinLoopLength(sampleRate: number = SingleUseVoice.sampleRate) {
//   if (
//     SingleUseVoice.minLoopLengthInSamples === undefined ||
//     sampleRate !== SingleUseVoice.sampleRate
//   ) {
//     SingleUseVoice.minLoopLengthInSamples =
//       SingleUseVoice.calculateMinLoopLength(sampleRate);
//   }
// }
// testloop();

// function calculateLoopLength(sampleRate: number, frequency: number): number {
//   return Math.round(sampleRate / frequency);
// }

// // Function to calculate frequency of C for a given octave
// function getCFrequency(octave: number): number {
//   // C4 (middle C) is 261.63 Hz
//   const C4_FREQUENCY = 261.63;
//   // Each octave doubles the frequency
//   return C4_FREQUENCY * Math.pow(2, octave - 4);
// }

// // Sample rate
// const SAMPLE_RATE = 48000;

// function testloop() {
//   // Calculate and print loop lengths for C notes from C0 to C8
//   for (let octave = 0; octave <= 8; octave++) {
//     const frequency = getCFrequency(octave);
//     const loopLength = calculateLoopLength(SAMPLE_RATE, frequency);
//     console.log(
//       `C${octave}: frequency = ${frequency.toFixed(
//         2
//       )} Hz, loop length = ${loopLength} samples`
//     );
//   }
// }

// updateLoopPoints(updated: Partial<Sample_settings> = this.settings) {
//   if (updated.loopStart) {
//     this.source.loopStart = snapToNearestZeroCrossing(
//       updated.loopStart,
//       SingleUseVoice.zeroCrossings.get(this.sampleId) ?? []
//     );
//   }
//   if (updated.loopEnd) {
//     this.source.loopEnd = snapToNearestZeroCrossing(
//       updated.loopEnd,
//       SingleUseVoice.zeroCrossings.get(this.sampleId) ?? []
//     );
//   }
// }

// loopEnvelope() {
//   if (
//     this.source.loop &&
//     (SingleUseVoice.globalLoop || this.voiceLoopLock)
//   ) {
//     const loopDuration = this.sample_settings.loopEnd - this.sample_settings.loopStart;
//     this.voiceGain.gain.setTargetAtTime(
//       this.sample_settings.loopVolume,
//       this.trigger + this.held + this.sample_settings.releaseTime,
//       this.sample_settings.attackTime
//     );
//     this.voiceGain.gain.setTargetAtTime(
//       0,
//       this.now() + this.held + this.sample_settings.releaseTime,
//       this.sample_settings.releaseTime
//     );
//     // this.held = this.now() - this.trigger;
//     console.log('looping:', this.held);
//   } else {
//   this.triggerRelease();
//   }
// }

// triggerAttack(offset: number = 0) {
//   this.trigger = this.now() + offset;
//   console.log('play:', this);
//   this.voiceGain.gain.setValueAtTime(0, this.trigger);

//   this.voiceGain.gain.linearRampToValueAtTime(
//     this.sample_settings.sampleVolume,
//     this.trigger + this.sample_settings.attackTime
//   );
//   if (SingleUseVoice.globalLoop || this.voiceLoopLock) {
//     this.triggerRelease(this.held);
//   }
// }

// triggerRelease(offset: number = 0) {
//   if (this.trigger < 0) return;
//   this.held = this.now() - this.trigger; // round
//   console.log('held:', this.held);

//   this.voiceGain.gain.setValueAtTime(
//     this.voiceGain.gain.value,
//     this.now() + offset
//   );
//   this.voiceGain.gain.linearRampToValueAtTime(
//     0,
//     this.now() + offset + this.sample_settings.releaseTime
//   );

//   if (!(SingleUseVoice.globalLoop || this.voiceLoopLock)) {
//     this.source.loop = false;
//     this.source.stop(this.now() + offset + this.sample_settings.releaseTime);
//   } else {
//     this.triggerAttack(offset + this.sample_settings.releaseTime);
//   }
// }
